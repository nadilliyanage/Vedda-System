{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "528e133c",
      "metadata": {
        "id": "528e133c"
      },
      "source": [
        "# üéôÔ∏è Vedda ASR Model Training - Google Colab\n",
        "\n",
        "Train a custom speech recognition model for Vedda language using Whisper on Google Colab with FREE GPU!\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT:** Before running this notebook:\n",
        "1. Click \"Runtime\" ‚Üí \"Change runtime type\" ‚Üí Select \"GPU (T4)\" ‚Üí \"Save\"\n",
        "2. Have your Vedda audio files ready (WAV format, 16kHz recommended)\n",
        "3. Create a folder `vedda-asr-dataset` in your Google Drive with audio files and transcriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cecae34",
      "metadata": {
        "id": "4cecae34"
      },
      "source": [
        "## üìã Step 1: Setup Environment\n",
        "\n",
        "Install all required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df27347",
      "metadata": {
        "id": "1df27347"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: GPU not available. Training will be very slow!\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cd0486",
      "metadata": {
        "id": "44cd0486"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "print(\"üîß Installing dependencies...\")\n",
        "!pip install -q openai-whisper transformers datasets evaluate jiwer librosa soundfile scikit-learn tqdm tensorboard accelerate\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9951924e",
      "metadata": {
        "id": "9951924e"
      },
      "source": [
        "## üìÇ Step 2: Mount Google Drive\n",
        "\n",
        "Connect your Google Drive to access your Vedda audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a012404",
      "metadata": {
        "id": "5a012404"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted!\")\n",
        "\n",
        "# Check if dataset folder exists\n",
        "dataset_dir = '/content/drive/MyDrive/vedda-asr-dataset'\n",
        "if os.path.exists(dataset_dir):\n",
        "    print(f\"‚úÖ Dataset found at: {dataset_dir}\")\n",
        "    print(f\"\\nüìÅ Contents:\")\n",
        "    for item in os.listdir(dataset_dir):\n",
        "        print(f\"   - {item}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Dataset folder not found at: {dataset_dir}\")\n",
        "    print(f\"\\nüìã Please create the folder in Google Drive and upload your audio files:\")\n",
        "    print(f\"   1. Go to Google Drive\")\n",
        "    print(f\"   2. Create folder 'vedda-asr-dataset'\")\n",
        "    print(f\"   3. Upload audio files (WAV format)\")\n",
        "    print(f\"   4. Create 'transcriptions.json' with transcriptions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec1b67c",
      "metadata": {
        "id": "9ec1b67c"
      },
      "source": [
        "## üìù Step 3: Prepare Dataset\n",
        "\n",
        "Create training and testing datasets from your audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bada29d0",
      "metadata": {
        "id": "bada29d0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Install noisereduce if not already installed\n",
        "print(\"üîß Installing audio processing tools...\")\n",
        "!pip install -q noisereduce\n",
        "import noisereduce as nr\n",
        "\n",
        "# Create project structure\n",
        "project_dir = '/content/vedda-asr'\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "processed_dir = os.path.join(data_dir, 'processed')\n",
        "models_dir = os.path.join(project_dir, 'models')\n",
        "\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Project structure created:\")\n",
        "print(f\"   {project_dir}/\")\n",
        "print(f\"   ‚îú‚îÄ‚îÄ data/\")\n",
        "print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ processed/\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb17dea1",
      "metadata": {
        "id": "cb17dea1"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import noisereduce as nr\n",
        "\n",
        "def prepare_dataset():\n",
        "    \"\"\"\n",
        "    Prepare dataset from audio files in Google Drive with noise cancellation\n",
        "\n",
        "    Expected structure in Google Drive (vedda-asr-dataset/):\n",
        "    - audio/\n",
        "      - vedda_001.wav\n",
        "      - vedda_002.wav\n",
        "      - ...\n",
        "    - transcriptions.json (format: {\"vedda_001\": \"‡∑Ñ‡∑ô‡∂Ω‡∑ù\", \"vedda_002\": \"‡∂Ü‡∂∫‡∑î‡∂∂‡∑ù‡∑Ä‡∂±‡∑ä\", ...})\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_dir = '/content/drive/MyDrive/vedda-asr-dataset'\n",
        "    audio_dir = os.path.join(dataset_dir, 'audio')\n",
        "    transcriptions_file = os.path.join(dataset_dir, 'transcriptions.json')\n",
        "\n",
        "    # Check if files exist\n",
        "    if not os.path.exists(audio_dir):\n",
        "        print(f\"‚ùå Audio directory not found: {audio_dir}\")\n",
        "        print(f\"\\nüìã Create this structure in Google Drive:\")\n",
        "        print(f\"   vedda-asr-dataset/\")\n",
        "        print(f\"   ‚îú‚îÄ‚îÄ audio/\")\n",
        "        print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ vedda_001.wav\")\n",
        "        print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ vedda_002.wav\")\n",
        "        print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ ...\")\n",
        "        print(f\"   ‚îî‚îÄ‚îÄ transcriptions.json\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(transcriptions_file):\n",
        "        print(f\"‚ùå Transcriptions file not found: {transcriptions_file}\")\n",
        "        print(f\"\\nüìã Create transcriptions.json with format:\")\n",
        "        print(f'   {{\"vedda_001\": \"‡∑Ñ‡∑ô‡∂Ω‡∑ù\", \"vedda_002\": \"‡∂Ü‡∂∫‡∑î‡∂∂‡∑ù‡∑Ä‡∂±‡∑ä\"}}')\n",
        "        return None\n",
        "\n",
        "    # Load transcriptions\n",
        "    with open(transcriptions_file, 'r', encoding='utf-8') as f:\n",
        "        transcriptions = json.load(f)\n",
        "\n",
        "    print(f\"üìÇ Found {len(transcriptions)} transcriptions\")\n",
        "\n",
        "    # Process audio files\n",
        "    processed_entries = []\n",
        "    audio_files = [f for f in os.listdir(audio_dir) if f.lower().endswith(('.wav', '.mp3', '.ogg', '.flac'))]\n",
        "\n",
        "    print(f\"üîß Processing {len(audio_files)} audio files with noise cancellation...\")\n",
        "\n",
        "    for audio_file in tqdm(audio_files):\n",
        "        audio_path = os.path.join(audio_dir, audio_file)\n",
        "        file_id = Path(audio_file).stem\n",
        "\n",
        "        # Get transcription\n",
        "        if file_id not in transcriptions:\n",
        "            print(f\"‚ö†Ô∏è  Skipping {audio_file} (no transcription)\")\n",
        "            continue\n",
        "\n",
        "        transcription = transcriptions[file_id]\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
        "\n",
        "            # Resample to 16kHz\n",
        "            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "\n",
        "            # ===== NOISE CANCELLATION =====\n",
        "            print(f\"  üîá Removing noise from {audio_file}...\")\n",
        "            audio = nr.reduce_noise(\n",
        "                y=audio,\n",
        "                sr=16000,\n",
        "                stationary=True,          # Remove stationary background noise\n",
        "                prop_decrease=1.0,        # Aggressiveness (0-1)\n",
        "                chunk_size=600000,        # Memory-efficient chunking\n",
        "                padding=30000,\n",
        "                freq_mask_smooth_hz=500,  # Smooth frequency mask\n",
        "                time_mask_smooth_ms=50    # Smooth time mask\n",
        "            )\n",
        "\n",
        "            # Normalize\n",
        "            audio = librosa.util.normalize(audio)\n",
        "\n",
        "            # Trim silence\n",
        "            audio, _ = librosa.effects.trim(audio, top_db=25)\n",
        "\n",
        "            # Pre-emphasis\n",
        "            audio = librosa.effects.preemphasis(audio)\n",
        "\n",
        "            # Final normalize\n",
        "            audio = librosa.util.normalize(audio)\n",
        "\n",
        "            duration = len(audio) / 16000\n",
        "            if duration < 0.5:\n",
        "                continue\n",
        "\n",
        "            # Save processed audio\n",
        "            output_path = os.path.join(processed_dir, audio_file)\n",
        "            sf.write(output_path, audio, 16000)\n",
        "\n",
        "            processed_entries.append({\n",
        "                'audio_path': output_path,\n",
        "                'transcription': transcription,\n",
        "                'duration': duration,\n",
        "                'noise_cancelled': True\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error processing {audio_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n‚úÖ Processed {len(processed_entries)} audio files (with noise cancellation)\")\n",
        "\n",
        "    total_duration = sum(e['duration'] for e in processed_entries)\n",
        "    print(f\"‚è±Ô∏è  Total duration: {total_duration/60:.1f} minutes\")\n",
        "\n",
        "    if total_duration < 600:  # Less than 10 minutes\n",
        "        print(f\"‚ö†Ô∏è  WARNING: Dataset is small (1 hour recommended)\")\n",
        "\n",
        "    return processed_entries\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = prepare_dataset()\n",
        "\n",
        "if dataset is None:\n",
        "    print(\"\\n‚ö†Ô∏è  Please set up your dataset first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b2ef754",
      "metadata": {
        "id": "8b2ef754"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train/test\n",
        "if dataset:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    train_data, test_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "    print(f\"üìä Dataset split:\")\n",
        "    print(f\"   Train: {len(train_data)} samples ({sum(e['duration'] for e in train_data)/60:.1f} min)\")\n",
        "    print(f\"   Test:  {len(test_data)} samples ({sum(e['duration'] for e in test_data)/60:.1f} min)\")\n",
        "\n",
        "    # Save datasets\n",
        "    train_file = os.path.join(data_dir, 'train_dataset.json')\n",
        "    test_file = os.path.join(data_dir, 'test_dataset.json')\n",
        "\n",
        "    with open(train_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump({'data': train_data}, f, ensure_ascii=False)\n",
        "\n",
        "    with open(test_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump({'data': test_data}, f, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n‚úÖ Datasets saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34729ad",
      "metadata": {
        "id": "a34729ad"
      },
      "source": [
        "## üéì Step 4: Train Whisper Model\n",
        "\n",
        "Fine-tune OpenAI Whisper on your Vedda data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850e14ab",
      "metadata": {
        "id": "850e14ab"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "from datasets import Dataset, Audio\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "print(\"‚úÖ Data structures ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a0a35c",
      "metadata": {
        "id": "52a0a35c"
      },
      "outputs": [],
      "source": [
        "# Load model and processor\n",
        "print(\"·Äí·Ä¢ Loading Whisper-small model...\")\n",
        "\n",
        "model_name = \"openai/whisper-small\"\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    model_name,\n",
        "    language=\"Sinhala\",\n",
        "    task=\"transcribe\"\n",
        ")\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Correct way to modify generation settings in newer transformers versions\n",
        "model.generation_config.forced_decoder_ids = None\n",
        "model.generation_config.suppress_tokens = []\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_name}\")\n",
        "print(f\"·Äí·Ä† Parameters: {model.num_parameters() / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f772385",
      "metadata": {
        "id": "3f772385"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "print(\"üìÇ Loading datasets...\")\n",
        "\n",
        "# Define paths (in case Step 3 cells weren't executed sequentially)\n",
        "data_dir = '/content/vedda-asr/data'\n",
        "train_file = os.path.join(data_dir, 'train_dataset.json')\n",
        "test_file = os.path.join(data_dir, 'test_dataset.json')\n",
        "\n",
        "# Check if dataset files exist\n",
        "if not os.path.exists(train_file) or not os.path.exists(test_file):\n",
        "    print(f\"‚ùå Error: Dataset files not found!\")\n",
        "    print(f\"   Train file: {train_file} ({'‚úÖ' if os.path.exists(train_file) else '‚ùå'})\")\n",
        "    print(f\"   Test file: {test_file} ({'‚úÖ' if os.path.exists(test_file) else '‚ùå'})\")\n",
        "    print(f\"\\nüìã Please run Step 3 cells in order:\")\n",
        "    print(f\"   1. 'Prepare dataset' cell\")\n",
        "    print(f\"   2. 'Split dataset into train/test' cell\")\n",
        "else:\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_json = json.load(f)\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        test_json = json.load(f)\n",
        "\n",
        "    train_dataset = Dataset.from_dict({\n",
        "        'audio': [x['audio_path'] for x in train_json['data']],\n",
        "        'transcription': [x['transcription'] for x in train_json['data']]\n",
        "    })\n",
        "\n",
        "    test_dataset = Dataset.from_dict({\n",
        "        'audio': [x['audio_path'] for x in test_json['data']],\n",
        "        'transcription': [x['transcription'] for x in test_json['data']]\n",
        "    })\n",
        "\n",
        "    train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "    test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "    print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
        "    print(f\"‚úÖ Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f17fad5",
      "metadata": {
        "id": "2f17fad5"
      },
      "outputs": [],
      "source": [
        "# Prepare datasets\n",
        "print(\"üîß Preparing datasets for training...\")\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    input_features = processor.feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"]\n",
        "    ).input_features[0]\n",
        "    labels = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return {\n",
        "        \"input_features\": input_features,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "train_dataset_proc = train_dataset.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "test_dataset_proc = test_dataset.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Datasets prepared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6cb588",
      "metadata": {
        "id": "1f6cb588"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "output_dir = os.path.join(models_dir, 'whisper-vedda')\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    logging_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    save_total_limit=3,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[\"tensorboard\"],\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training configuration ready\")\n",
        "print(f\"   Output: {output_dir}\")\n",
        "print(f\"   Epochs: 10\")\n",
        "print(f\"   Batch size: 8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0b882e",
      "metadata": {
        "id": "5e0b882e"
      },
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "def compute_metrics(pred):\n",
        "    from evaluate import load\n",
        "    wer_metric = load(\"wer\")\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_proc,\n",
        "    eval_dataset=test_dataset_proc,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Trainer ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a755a4",
      "metadata": {
        "id": "53a755a4"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üöÄ STARTING TRAINING\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ TRAINING COMPLETE\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d2bba3",
      "metadata": {
        "id": "a8d2bba3"
      },
      "source": [
        "## üìä Step 5: Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fa253a",
      "metadata": {
        "id": "66fa253a"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"üìä Evaluating on test set...\\n\")\n",
        "\n",
        "results = trainer.evaluate(test_dataset_proc)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üìà EVALUATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nWord Error Rate (WER): {results['eval_wer']*100:.2f}%\")\n",
        "\n",
        "if results['eval_wer'] < 0.10:\n",
        "    quality = \"üåü Excellent (Production-ready)\"\n",
        "elif results['eval_wer'] < 0.20:\n",
        "    quality = \"‚úÖ Good (Usable)\"\n",
        "elif results['eval_wer'] < 0.30:\n",
        "    quality = \"‚ö†Ô∏è  Fair (Needs more data)\"\n",
        "else:\n",
        "    quality = \"‚ùå Poor (Collect more data)\"\n",
        "\n",
        "print(f\"Quality: {quality}\")\n",
        "print(f\"{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c997c483",
      "metadata": {
        "id": "c997c483"
      },
      "source": [
        "## üíæ Step 6: Save and Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51885e9b",
      "metadata": {
        "id": "51885e9b"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_dir = os.path.join(models_dir, 'whisper-vedda-final')\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "print(f\"üíæ Saving final model...\")\n",
        "model.save_pretrained(final_dir)\n",
        "processor.save_pretrained(final_dir)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {final_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48380513",
      "metadata": {
        "id": "48380513"
      },
      "outputs": [],
      "source": [
        "# Create ZIP file for download\n",
        "import shutil\n",
        "\n",
        "print(f\"·ÄÅ·Ä∞ Creating download package...\")\n",
        "zip_path = os.path.join(project_dir, 'vedda-asr-model')\n",
        "shutil.make_archive(zip_path, 'zip', final_dir)\n",
        "\n",
        "zip_file = zip_path + '.zip'\n",
        "size_mb = os.path.getsize(zip_file) / 1e6\n",
        "\n",
        "print(f\"\\n‚úÖ Model packaged for download!\")\n",
        "print(f\"   File: vedda-asr-model.zip\")\n",
        "print(f\"   Size: {size_mb:.1f} MB\")\n",
        "\n",
        "print(f\"\\n·Ä±·Ä≥ The file is available in Colab's file browser (left panel)\")\n",
        "print(f\"   Right-click ‚Üí Download\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5533d825",
      "metadata": {
        "id": "5533d825"
      },
      "source": [
        "## üß™ Step 7: Test Trained Model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ca6eb9",
      "metadata": {
        "id": "42ca6eb9"
      },
      "outputs": [],
      "source": [
        "# Test model on a sample audio\n",
        "import librosa\n",
        "\n",
        "print(\"üé§ Testing trained model...\\n\")\n",
        "\n",
        "# Get first test sample\n",
        "test_sample = test_json['data'][0]\n",
        "audio_path = test_sample['audio_path']\n",
        "reference = test_sample['transcription']\n",
        "\n",
        "# Load audio\n",
        "audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "# Process with processor\n",
        "input_features = processor(\n",
        "    audio,\n",
        "    sampling_rate=16000,\n",
        "    return_tensors=\"pt\"\n",
        ").input_features\n",
        "\n",
        "# Generate prediction\n",
        "with torch.no_grad():\n",
        "    predicted_ids = model.generate(input_features)\n",
        "\n",
        "prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(f\"üìù Sample Test Result:\")\n",
        "print(f\"\\n   Reference:  {reference}\")\n",
        "print(f\"   Predicted:  {prediction}\")\n",
        "print(f\"   Match: {'‚úÖ YES' if reference == prediction else '‚ùå NO'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe014cc",
      "metadata": {
        "id": "ffe014cc"
      },
      "source": [
        "## üìã Summary\n",
        "\n",
        "### What you learned:\n",
        "1. ‚úÖ Set up GPU environment in Colab\n",
        "2. ‚úÖ Loaded Vedda audio data from Google Drive\n",
        "3. ‚úÖ Prepared dataset for training\n",
        "4. ‚úÖ Fine-tuned Whisper model on Vedda speech\n",
        "5. ‚úÖ Evaluated model performance (WER)\n",
        "6. ‚úÖ Saved and packaged trained model\n",
        "\n",
        "### Next steps:\n",
        "1. Download the trained model (vedda-asr-model.zip)\n",
        "2. Extract it locally\n",
        "3. Integrate with your speech service\n",
        "4. For better accuracy: Collect more data and retrain\n",
        "\n",
        "### Performance improvements:\n",
        "- **More data:** 1 hour ‚Üí Better accuracy\n",
        "- **More epochs:** 10 ‚Üí 15 or 20\n",
        "- **Larger model:** small ‚Üí base or medium\n",
        "- **Lower learning rate:** 1e-5 ‚Üí 5e-6\n",
        "\n",
        "---\n",
        "\n",
        "**Happy training! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}