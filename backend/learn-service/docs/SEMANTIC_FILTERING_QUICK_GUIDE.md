# Semantic Filtering - Quick Visual Guide

## One-Page Overview

### What is Semantic Filtering?

Instead of just keyword matching, semantic filtering finds documents based on **meaning** using embeddings (vectors of numbers).

```
âŒ KEYWORD MATCHING:
  Query: "question words"
  Found: Only docs with exact words "question" + "words"
  Problem: Misses relevant docs that mean the same thing

âœ… SEMANTIC FILTERING:
  Query: "how to ask what in Vedda"
  Found: All docs about question formation, mokadda, koheda, etc.
  Benefit: Finds by meaning, not just keywords
```

---

## 5-Minute Understanding

### 1. What is an Embedding?

**Simple Explanation:**
An embedding is a set of numbers that represent the meaning of text.

```
Text: "What is the Vedda word for honey?"

Embedding:
[0.0076, 0.0023, 0.0085, 0.0245, -0.0157, ...]
          â†‘ 1536 numbers total
          
Each number captures part of the meaning:
- Num 1: "Is this about language?" (0.0076)
- Num 2: "Is this about questions?" (0.0023)
- Num 3: "Is this about Vedda?" (0.0085)
...
- Num 1536: (...) = 0.456
```

**Why 1536?** OpenAI's neural network uses 1536 dimensions to capture rich semantic meaning.

---

### 2. How Are Embeddings Generated?

```
Step 1: Take Knowledge Document
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Content: "mokadda" means "what"     â”‚
â”‚ Example: "Mokadda karanne?"         â”‚
â”‚ Skills: ["question_forms"]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â†“ Preparation (combine all info)

Step 2: Send to OpenAI API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "mokadda means what | Example:      â”‚
â”‚  Mokadda karanne? | Skills:         â”‚
â”‚  question_forms"                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â†“ Neural Network Processing

Step 3: Get Back Embedding
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [0.0076, 0.0023, ..., 0.456]       â”‚
â”‚ (1536 numbers representing meaning) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â†“ Store in Database

Step 4: Save to MongoDB
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                   â”‚
â”‚   "_id": ObjectId(...),             â”‚
â”‚   "content": "mokadda means what",  â”‚
â”‚   "embedding": [0.0076, 0.0023...] â”‚
â”‚ }                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. How is Similarity Calculated?

**Cosine Similarity = How similar are two meaning-vectors?**

```
Document 1: "What is Vedda word for honey?"
Embedding:  [0.20, 0.85, 0.15]

Document 2: "How to say honey in Vedda?"
Embedding:  [0.21, 0.84, 0.16]

        â†“ Calculate angle between vectors

Cosine Similarity = 0.98 âœ“âœ“âœ“ VERY SIMILAR!

---

Document 3: "The weather is nice"
Embedding:  [0.01, 0.05, 0.92]

        â†“ Calculate angle between vectors

Cosine Similarity = 0.12 âœ— NOT SIMILAR
```

**Visual:**
```
     Vector A          Vector B          Vector C
     (similar)         (different)
       â†—                  â†—                  â†“
      /                  /                  |
     /                  /                   |
    /                  /                    |
   â†— Very close!      â†— Far apart!         | Very different!

Similarity = 0.98      Similarity = 0.12   Similarity = 0.05
(Close vectors)        (Far vectors)       (Opposite vectors)
```

---

### 4. How Does Filtering Work?

```
STEP 1: SYMBOLIC FILTER (Fast)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ All 55 knowledge docs            â”‚
â”‚ Filter: skill_tags = question_*  â”‚
â”‚ Filter: difficulty = beginner    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ Returns 4 candidates

STEP 2: SEMANTIC SEARCH (Accurate)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query: "Student confused about"  â”‚
â”‚        "question words"           â”‚
â”‚                                  â”‚
â”‚ Generate query embedding         â”‚
â”‚ Compare to 4 docs' embeddings    â”‚
â”‚ Get similarity scores:           â”‚
â”‚ Doc1: 0.87 âœ“âœ“âœ“âœ“âœ“               â”‚
â”‚ Doc2: 0.84 âœ“âœ“âœ“âœ“                â”‚
â”‚ Doc3: 0.72 âœ“âœ“âœ“                 â”‚
â”‚ Doc4: 0.23 âœ“                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ Returns ranked docs

STEP 3: BOOST FACTORS (Domain-specific)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Doc1 Score:                      â”‚
â”‚ â€¢ Similarity: 0.87 Ã— 5 = 4.35   â”‚
â”‚ â€¢ Error match: +3.0              â”‚
â”‚ â€¢ Weak skill match: +2.0         â”‚
â”‚ â€¢ Priority: +2.0                 â”‚
â”‚ = TOTAL: 11.35 ğŸ† #1             â”‚
â”‚                                  â”‚
â”‚ Doc2 Score:                      â”‚
â”‚ â€¢ Similarity: 0.84 Ã— 5 = 4.20   â”‚
â”‚ â€¢ Error match: +3.0              â”‚
â”‚ â€¢ Weak skill match: +2.0         â”‚
â”‚ â€¢ Priority: +1.0                 â”‚
â”‚ = TOTAL: 10.20 ğŸ¥ˆ #2             â”‚
â”‚                                  â”‚
â”‚ Doc3 Score:                      â”‚
â”‚ â€¢ Similarity: 0.72 Ã— 5 = 3.60   â”‚
â”‚ â€¢ Error match: +3.0              â”‚
â”‚ â€¢ Weak skill match: +0.0         â”‚
â”‚ â€¢ Priority: +0.0                 â”‚
â”‚ = TOTAL: 6.60 ğŸ¥‰ #3              â”‚
â”‚                                  â”‚
â”‚ Doc4 Score:                      â”‚
â”‚ â€¢ Similarity: 0.23 Ã— 5 = 1.15   â”‚
â”‚ â€¢ Error match: +0.0              â”‚
â”‚ â€¢ = TOTAL: 1.15 âŒ Not relevant   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ Returns Top-3 Docs

STEP 4: BUILD CONTEXT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ === RELEVANT GRAMMAR RULES ===   â”‚
â”‚                                  â”‚
â”‚ â€¢ "mokadda" means "what"         â”‚
â”‚   Example: Mokadda karanne?      â”‚
â”‚                                  â”‚
â”‚ â€¢ "koheda" means "where"         â”‚
â”‚   Example: Koheda yanava?        â”‚
â”‚                                  â”‚
â”‚ â€¢ Question word order rule       â”‚
â”‚   Example: Kauda enne?           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ Pass to LLM

STEP 5: GENERATE FEEDBACK
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Student said: "Koheda karanne?"  â”‚
â”‚ Correct: "Mokadda karanne?"      â”‚
â”‚                                  â”‚
â”‚ Feedback: "You used 'koheda'     â”‚
â”‚ (where) instead of 'mokadda'     â”‚
â”‚ (what). 'Mokadda' asks about     â”‚
â”‚ things or actions..."            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Real Numbers Example

### Complete Flow

```
USER INTERACTION:
  Student: "What is the Vedda word for honey?"
  Student Answer: "pinida"
  Correct Answer: "piniya"

SYSTEM PROCESSING:

1ï¸âƒ£ Extract Context
   Weak Skills: ["vocabulary"]
   Common Errors: ["wrong_vocabulary"]

2ï¸âƒ£ Symbolic Filter
   Query knowledge for skill="vocabulary"
   Found: 8 documents

3ï¸âƒ£ Generate Query Embedding
   Query: "honey vs pinida error in vocabulary"
   Embedding: [-0.012, 0.234, ..., 0.156] (1536 numbers)

4ï¸âƒ£ Calculate Similarities
   Doc1 (piniya=honey): 0.92 â˜…â˜…â˜…â˜…â˜…
   Doc2 (numbers): 0.45 â˜…â˜…
   Doc3 (colors): 0.38 â˜…â˜…
   Doc4 (greetings): 0.15 â˜…
   Doc5 (actions): 0.28 â˜…
   Doc6 (body parts): 0.51 â˜…â˜…â˜…
   Doc7 (family): 0.33 â˜…
   Doc8 (animals): 0.67 â˜…â˜…â˜…â˜…

5ï¸âƒ£ Apply Boosts
   Doc1:
     Similarity: 0.92 Ã— 5 = 4.60
     + Error match: 3.0
     + Weak skill: 2.0
     = TOTAL: 9.60 ğŸ†

   Doc8 (animals):
     Similarity: 0.67 Ã— 5 = 3.35
     + Error match: 0.0
     + Weak skill: 0.0
     = TOTAL: 3.35

6ï¸âƒ£ Top-3 Results
   âœ… Doc1: "piniya (honey)" - Score 9.60
   âœ… Doc6: "body parts" - Score 5.51
   âœ… Doc8: "animals" - Score 3.35

7ï¸âƒ£ Generate Feedback
   "Good effort! The Vedda word for
   honey is 'piniya' not 'pinida'.
   Remember the double 'i' ending."
```

---

## Key Formulas

### Cosine Similarity Formula

```
       A Â· B
sim = â”€â”€â”€â”€â”€â”€â”€â”€â”€
      ||A|| ||B||

Where:
  A Â· B = sum of (A[i] Ã— B[i]) for all i
  ||A|| = sqrt(sum of A[i]Â²)
  ||B|| = sqrt(sum of B[i]Â²)

Result: Number between 0 and 1
  1.0 = Identical meaning
  0.5 = Somewhat similar
  0.0 = Completely different
```

### Boost Score Formula

```
final_score = (similarity Ã— 5) + boost_factors

Where boost_factors include:
  - Error type match: +3.0
  - Exercise type match: +2.0
  - Weak skill match: +2.0
  - Document priority: +value
  - Effectiveness rate: +help_rateÃ—1.5
```

---

## Why This Matters

### Traditional Keyword Search
```
Query: "what question"
Match: Docs containing "what" AND "question"
Problem: Misses "mokadda" or "how to ask"
Result: Limited, rigid
```

### Semantic Search (Our System)
```
Query: "what question"
Match: Docs about asking questions, mokadda, koheda, etc.
Benefit: Understands meaning, finds related concepts
Result: Rich, flexible, learner-focused
```

---

## Implementation Summary

| Step | Input | Process | Output |
|------|-------|---------|--------|
| 1 | Text | Preparation | Combined text |
| 2 | Combined text | OpenAI API | Embedding (1536 numbers) |
| 3 | Embedding | Store | Document in DB |
| 4 | User query | Generate embedding | Query embedding |
| 5 | Query + Docs | Cosine similarity | Similarity scores |
| 6 | Similarity | Apply boosts | Final scores |
| 7 | Scores | Sort & filter | Top-k docs |
| 8 | Docs | Format | Context string |
| 9 | Context | LLM | Feedback |

---

## Quick Setup

```bash
# 1. Generate embeddings for all knowledge docs
python populate_embeddings.py --populate

# 2. Verify 100% coverage
python populate_embeddings.py --verify

# 3. Test the system
python test_hybrid_rag.py --test 4  # Test hybrid retrieval

# 4. Monitor performance
curl http://localhost:5006/api/learn/admin/rag/system-stats
```

---

**For detailed technical explanation, see:** `SEMANTIC_FILTERING_DETAILED.md`

---

**Document Version:** 1.0  
**Date:** February 21, 2026  
**System:** Vedda Language Learning Platform

